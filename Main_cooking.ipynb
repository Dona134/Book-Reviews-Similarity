{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.ml.feature import MinHashLSH, HashingTF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BooksReviewSimilarity\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cooking = spark.read.csv(\"df_cooking.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Books Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+\n",
      "|           Title|         review/text|             User_id|\n",
      "+----------------+--------------------+--------------------+\n",
      "|Alaska Sourdough|I have been using...|       AC58Z72OB2DDX|\n",
      "|Alaska Sourdough|My poor dogeared,...|      A3CNQIKVTG9QYO|\n",
      "|Alaska Sourdough|As a former Alask...|      A2UMP9TJTJ6A6B|\n",
      "|Alaska Sourdough|\"For those of us ...| and baking soda ...|\n",
      "|Alaska Sourdough|Make the most sub...|      A22T74YNRM8NTK|\n",
      "+----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cooking.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22434, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11232, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample the dataset (50%)\n",
    "df_cooking = df_cooking.sample(0.5, seed=123)\n",
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9694, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "# drop rows where User_id is not all caps and that has spaces\n",
    "df_cooking = df_cooking.filter(\n",
    "\t(col(\"User_id\") == upper(col(\"User_id\"))) & (~col(\"User_id\").contains(\" \"))\n",
    ")\n",
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring that a user only has one entry per book\n",
    "df_cooking = df_cooking.dropDuplicates(\n",
    "    subset=[\"Title\", \"User_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_cooking.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(lower(col(\"review/text\")), r'[^\\w\\s]', '')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|\n",
      "+--------------------+--------------------+--------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|\n",
      "|\"Cooking with Sei...|Cookbook on Cooki...| AFYO0DI7QNLAQ|cookbook on cooki...|\n",
      "|\"Mexican Everyday...|Whether you buy t...|A149SA6L13J3E8|whether you buy t...|\n",
      "|\"Mexican Everyday...|I've enjoyed watc...|A1RROHFHHCIVM7|ive enjoyed watch...|\n",
      "|\"Mexican Everyday...|I bought this gif...|A2O6JIFMT8VG7D|i bought this gif...|\n",
      "+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# 1. Define custom stopwords\n",
    "custom_stopwords = [\"book\", \"read\", \"recipe\", \"good\", \"great\"]\n",
    "\n",
    "# Combine with default stopwords\n",
    "default_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "all_stopwords = default_stopwords + custom_stopwords\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_text\", stopWords=all_stopwords)\n",
    "df_no_stop = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|[the, book, was, ...|[delivered, speed...|[delivered speedy...|\n",
      "|\"Cooking with Sei...|Cookbook on Cooki...| AFYO0DI7QNLAQ|cookbook on cooki...|[cookbook, on, co...|[cookbook, cookin...|[cookbook cooking...|\n",
      "|\"Mexican Everyday...|Whether you buy t...|A149SA6L13J3E8|whether you buy t...|[whether, you, bu...|[whether, buy, fr...|[whether buy, buy...|\n",
      "|\"Mexican Everyday...|I've enjoyed watc...|A1RROHFHHCIVM7|ive enjoyed watch...|[ive, enjoyed, wa...|[ive, enjoyed, wa...|[ive enjoyed, enj...|\n",
      "|\"Mexican Everyday...|I bought this gif...|A2O6JIFMT8VG7D|i bought this gif...|[i, bought, this,...|[bought, gift, hu...|[bought gift, gif...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create word-level shingles (n-grams)\n",
    "def create_word_shingles(words, n=2):  \n",
    "    if len(words) < n:\n",
    "        return [\" \".join(words)]\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "# Define UDF\n",
    "shingle_udf = udf(lambda words: create_word_shingles(words, n=2), ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to generate shingles\n",
    "df_shingled = df_no_stop.withColumn(\"shingles\", shingle_udf(col(\"filtered_text\")))\n",
    "\n",
    "df_shingled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors (using hashing trick)\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=1 << 20)  # 2^20 features  #transform a sequence of terms (like words or shingles) into a fixed-length feature vector.\n",
    "featurized_data = hashing_tf.transform(df_shingled)   #each bigram gets hashed to an index between 0 and 1,048,575 and the value = freq of occurence (sparce vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|            features|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|[the, book, was, ...|[delivered, speed...|[delivered speedy...|(1048576,[5006,39...|\n",
      "|\"Cooking with Sei...|Cookbook on Cooki...| AFYO0DI7QNLAQ|cookbook on cooki...|[cookbook, on, co...|[cookbook, cookin...|[cookbook cooking...|(1048576,[6129,35...|\n",
      "|\"Mexican Everyday...|Whether you buy t...|A149SA6L13J3E8|whether you buy t...|[whether, you, bu...|[whether, buy, fr...|[whether buy, buy...|(1048576,[73610,7...|\n",
      "|\"Mexican Everyday...|I've enjoyed watc...|A1RROHFHHCIVM7|ive enjoyed watch...|[ive, enjoyed, wa...|[ive, enjoyed, wa...|[ive enjoyed, enj...|(1048576,[28827,2...|\n",
      "|\"Mexican Everyday...|I bought this gif...|A2O6JIFMT8VG7D|i bought this gif...|[i, bought, this,...|[bought, gift, hu...|[bought gift, gif...|(1048576,[28827,8...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = featurized_data.withColumn(\"Review_ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinHash\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=6)\n",
    "model = mh.fit(df_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar reviews\n",
    "similar_reviews = model.approxSimilarityJoin(\n",
    "    df_with_id, df_with_id, 0.8, distCol=\"JaccardDistance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = similar_reviews.filter(col(\"datasetA.Review_ID\") != col(\"datasetB.Review_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only one direction of each pair\n",
    "filtered_df = filtered_df.filter(col(\"datasetA.review_ID\") < col(\"datasetB.review_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude pairs with distance = 0\n",
    "filtered_df = filtered_df.filter(col(\"JaccardDistance\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = (\n",
    "    filtered_df\n",
    "    .orderBy(col(\"JaccardDistance\").asc())\n",
    "    .limit(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+-------------------+\n",
      "|review/text_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Title_A                                                                          |review/text_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Title_B                                                                                            |JaccardDistance    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+-------------------+\n",
      "|I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The Eggplant Parmesan recipe is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too, I bought 10 as gifts for family members. Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |High Fit, Low Fat Vegetarian                                                     |I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The eggplant parmesan recipe (Vegetarian cookbook) is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too (I bought 10 as gifts for family members)! Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                              |High Fit, Low Fat Vegetarian                                                                       |0.08510638297872342|\n",
      "|Mary Sue Miliken and Susan Feniger are two of the top chefs in LA. (And maybe the top female duo.) For those of us who grew up equating Mexican food with Tex-Mex, Miliken and Feniger's inventive take on traditional Mexican cuisine is a revelation. Their &quot;Border Grill&quot; in Santa Monica (4th and Broadway) is a noisy, splashy, foodie haven with superb drinks, a decent wine/beer list, and amazing food. For several years they also produced a fun and informational show on the Food network before that cable network went all-Emeril all-the-time. &quot;Cooking with Two Hot Tamales&quot; captures a lot of recipes and tips from the show. Many of the recipes herein one occasionally sees on Border Grill's menu. The house gucamole recipe is almost worth the price of the book on its own!As a cookbook, Two Hot Tamales is interesting, has an attractive layout, and, by the minimal standards of the genre, is well-written. Unlike their Mesa Mexicana, which I recomend only for the hobbyist chef with access to a decent Mexican grocer and time on his/her hands, Two Hot Tamales can be used on an everyday basis. Few of the recipes involve intensive prep work -- after all, they had to be prepared within the confines of a 30 minute TV show. Equally important for users outside the South-west, few of the recipes require specialized ingredients. Highly recommended.|Cooking with Too Hot Tamales : Recipes & Tips From Tv Food's Spiciest Cooking Duo|Mary Sue Miliken and Susan Feniger are two of the top chefs in LA. Their &quot;Border Grill&quot; in Santa Monica (4th and Broadway) is a noisy, splashy, foodie haven with superb drinks, a decent wine/beer list, and amazing food. Mesa Mexicana offers recipes that one might easily see on Border Grill's menu. For those of us who grew up equating Mexican food with Tex-Mex, Miliken and Feniger's inventive take on traditional Mexican cuisine is a revelation.As a cookbook, Mesa Mexicana is interesting, has an attractive layout, and, by the minimal standards of the genre, is well-written. One would not want to use it on an everyday basis. Many of the recipes involve a fairly intensive amount of prep work and/or require specialized ingredients. For the hobbyist chef with access to a decent Mexican grocer and time on his/her hands, however, it is an inspiring and provocative work. Highly recommended on that qualified basis.|Mesa Mexicana                                                                                      |0.6363636363636364 |\n",
      "|This cookbook is wonderful. The recipes are easy to follow and turn out great! Would recomend to anyone.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |The Hay Day Country Market Cookbook                                              |This cookbook is wonderful and the recipes are easy and not too hard to figure out. Glad I bought it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Weight Watchers Take-Out Tonight! : 150+ Restaurant Favorites to Make at Home--All 8 POINTS or Less|0.7272727272727273 |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3.select(\n",
    "    col(\"datasetA.review/text\").alias(\"review/text_A\"),\n",
    "    col(\"datasetA.Title\").alias(\"Title_A\"),\n",
    "    col(\"datasetB.review/text\").alias(\"review/text_B\"),\n",
    "    col(\"datasetB.Title\").alias(\"Title_B\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline loads Amazon book review data, cleans and preprocesses the review text, generates 5-character shingles (n-grams), and converts these into feature vectors using the hashing trick. It then applies MinHash Locality Sensitive Hashing (LSH) to find pairs of similar reviews based on Jaccard distance. Finally, it filters out duplicate and self-pairs, sorts by similarity, and displays the top similar review pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "# exclude rows where 'filtered_text' has less than 5 words\n",
    "featurized_data = featurized_data.filter(size(col(\"filtered_text\")) >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply IDF to get TF-IDF vectors\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "df_tfidf = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, applying TF-IDF in this context makes sense. After converting the text data into feature vectors using HashingTF (which captures term frequencies), using IDF adjusts these frequencies by reducing the weight of terms that appear frequently across many documents and increasing the weight of rarer, more distinctive terms. This results in feature vectors that better represent the unique content of each review, which is especially useful when comparing reviews for similarity. Using TF-IDF can improve the quality of similarity search and clustering compared to using raw term frequencies alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|            features|      tfidf_features|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|[the, book, was, ...|[delivered, speed...|[delivered speedy...|(1048576,[5006,39...|(1048576,[5006,39...|\n",
      "|\"Cooking with Sei...|Cookbook on Cooki...| AFYO0DI7QNLAQ|cookbook on cooki...|[cookbook, on, co...|[cookbook, cookin...|[cookbook cooking...|(1048576,[6129,35...|(1048576,[6129,35...|\n",
      "|\"Mexican Everyday...|Whether you buy t...|A149SA6L13J3E8|whether you buy t...|[whether, you, bu...|[whether, buy, fr...|[whether buy, buy...|(1048576,[73610,7...|(1048576,[73610,7...|\n",
      "|\"Mexican Everyday...|I've enjoyed watc...|A1RROHFHHCIVM7|ive enjoyed watch...|[ive, enjoyed, wa...|[ive, enjoyed, wa...|[ive enjoyed, enj...|(1048576,[28827,2...|(1048576,[28827,2...|\n",
      "|\"Mexican Everyday...|I bought this gif...|A2O6JIFMT8VG7D|i bought this gif...|[i, bought, this,...|[bought, gift, hu...|[bought gift, gif...|(1048576,[28827,8...|(1048576,[28827,8...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tfidf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = df_tfidf.withColumn(\"Review_ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Use LSH with Cosine-like projection (BucketedRandomProjectionLSH)\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.5, numHashTables=4)  #number of different “ways” you're looking for similarity.\n",
    "\n",
    "\n",
    "brp_model = brp.fit(df_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Self-join to find similar review pairs\n",
    "similar_pairs = brp_model.approxSimilarityJoin(df_with_id, df_with_id, threshold=5.0, distCol=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Filter out duplicates and self-pairs\n",
    "similar_pairs_filtered = similar_pairs \\\n",
    "    .filter(col(\"datasetA.Review_ID\") < col(\"datasetB.Review_ID\")) \\\n",
    "    .filter(col(\"datasetA.Review_ID\") != col(\"datasetB.Review_ID\")) \\\n",
    "    .select(\"datasetA.Title\", \"datasetB.Title\", \"datasetA.User_id\", \"datasetB.User_id\", \"datasetA.review/text\", \"datasetB.review/text\", \"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------+\n",
      "|                                                                                               Title|                                                                                               Title|       User_id|       User_id|                                                                                         review/text|                                                                                         review/text|distance|\n",
      "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------+\n",
      "|How to Make Love and Dinner at the Same Time: 200 Slow-Cooker Recipes to Heat Up the Bedroom Inst...|How to Make Love and Dinner at the Same Time: 200 Slow-Cooker Recipes to Heat Up the Bedroom Inst...|A33SO8M3622NF8| AM8L95T6F4QIV|I've had this book for three months and have rediscovered how perfect the slow cooker is for my f...|I've had this book for three months and have rediscovered how perfect the slow cooker is for my f...|     0.0|\n",
      "|                                                                      Indonesian Street Food Secrets|                                                                      Indonesian Street Food Secrets|A2W9XFN819J7UI|A3D6B3ZKFMFAD5|I love this book. Having traveled in Indonesia I found the foods particularly unique. Its great t...|I love this book. Having traveled in Indonesia I found the foods particularly unique. Its great t...|     0.0|\n",
      "|                                                                        High Fit, Low Fat Vegetarian|                                                                        High Fit, Low Fat Vegetarian|A2JDVB1LKLN97N| AK9FPASRM79OX|I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both...|I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for b...|     2.0|\n",
      "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_filtered.orderBy(\"distance\").show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exlude pairs with distance = 0\n",
    "similar_pairs_filtered = similar_pairs_filtered.filter(col(\"distance\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "|                                                                                         Title|                                                                                               Title|       User_id|       User_id|                                                                                         review/text|                                                                                         review/text|          distance|\n",
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "|                                                                  High Fit, Low Fat Vegetarian|                                                                        High Fit, Low Fat Vegetarian|A2JDVB1LKLN97N| AK9FPASRM79OX|I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both...|I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for b...|               2.0|\n",
      "|                                                       The All New All Purpose: Joy of Cooking|                                                All About Braising: The Art of Uncomplicated Cooking|A2PQTIF0WD7T3A| AO598PUMZ7CKU|                 My book arrived in a timely manner and as discribed. I would use this seller again.|Product arrived at a timely manner and the book is in good condition. Would purchase with the sam...|2.6457513110645907|\n",
      "|How to Grill: The Complete Illustrated Book of Barbecue Techniques, A Barbecue Bible! Cookbook|  This Can't Be Tofu!: 75 Recipes to Cook Something You Never Thought You Would--and Love Every Bite|A2ZPAJD0AJ0B66|A35NNPZP83H65F|                              this book is one of the best bq book on the market, i'm glad i got it.|            I'm so glad I got more recipes for tofu. It's so good for you. This is a wonderful book.|2.6457513110645907|\n",
      "|                      Jill Prescott's Ecole de Cuisine: Professional Cooking for the Home Chef|Saving Dinner the Low-Carb Way: Healthy Menus, Recipes, and the Shopping Lists That Will Keep the...|A1ZEHXQQLC9NB3|A191C036Y8W8CQ|This is a great cookbook. Everything I have made has been excellent. I have also given the book a...|           I have this and have also given it as gifts. For a busy family this is a great cool book.|2.6457513110645907|\n",
      "|                                                        Good Housekeeping Illustrated Cookbook|                                                       1000 Vegetarian Recipes From Around the World| A1ZGX2HV4L1JE| AYKWM682POY3J|                            this is best cookbook i have ever had. mine was falling apart. thank you|                     Whether you are vegetarian or not, this is the best cookbook I have ever owned.|2.6457513110645907|\n",
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_filtered.orderBy(\"distance\").show(5, truncate=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
