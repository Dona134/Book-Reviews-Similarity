{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.ml.feature import MinHashLSH, HashingTF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BooksReviewSimilarity\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cooking = spark.read.csv(\"df_cooking.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+\n",
      "|           Title|         review/text|             User_id|\n",
      "+----------------+--------------------+--------------------+\n",
      "|Alaska Sourdough|I have been using...|       AC58Z72OB2DDX|\n",
      "|Alaska Sourdough|My poor dogeared,...|      A3CNQIKVTG9QYO|\n",
      "|Alaska Sourdough|As a former Alask...|      A2UMP9TJTJ6A6B|\n",
      "|Alaska Sourdough|\"For those of us ...| and baking soda ...|\n",
      "|Alaska Sourdough|Make the most sub...|      A22T74YNRM8NTK|\n",
      "+----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cooking.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22434, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11217, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample the dataset (50%)\n",
    "# df_cooking = df_cooking.sample(0.5, seed=123)\n",
    "# df_cooking.count(), len(df_cooking.columns)\n",
    "\n",
    "\n",
    "total_rows = df_cooking.count()\n",
    "\n",
    "# Calculate the number of rows for the first half\n",
    "half_rows = total_rows // 2  \n",
    "\n",
    "# Subset the first half of the dataset\n",
    "df_cooking = df_cooking.limit(half_rows)\n",
    "\n",
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9706, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "# drop rows where User_id is not all caps and that has spaces\n",
    "df_cooking = df_cooking.filter(\n",
    "\t(col(\"User_id\") == upper(col(\"User_id\"))) & (~col(\"User_id\").contains(\" \"))\n",
    ")\n",
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that a user only has one entry per book\n",
    "df_cooking = df_cooking.dropDuplicates(\n",
    "    subset=[\"Title\", \"User_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_cooking.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(lower(col(\"review/text\")), r'[^\\w\\s]', '')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|\n",
      "+--------------------+--------------------+--------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|\n",
      "|\"Cooking with Sei...|The Book of Seita...|A1I3R6RQ3XI76B|the book of seita...|\n",
      "|\"Cooking with Sei...|it was what i nee...|A2SK6R2DPGY3P2|it was what i nee...|\n",
      "|\"Cooking with Sei...|\"As a vegetarian ...|A363UEL2VJNBX9|as a vegetarian i...|\n",
      "|\"Cooking with Sei...|Gluten has been a...|A3HRBWBN7DH9XV|gluten has been a...|\n",
      "+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define custom stopwords\n",
    "custom_stopwords = [\"book\", \"read\", \"recipe\", \"good\", \"great\"]\n",
    "\n",
    "# Combine with default stopwords\n",
    "default_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "all_stopwords = default_stopwords + custom_stopwords\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_text\", stopWords=all_stopwords)\n",
    "df_no_stop = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|[the, book, was, ...|[delivered, speed...|[delivered speedy...|\n",
      "|\"Cooking with Sei...|The Book of Seita...|A1I3R6RQ3XI76B|the book of seita...|[the, book, of, s...|[seitan, instruct...|[seitan instructi...|\n",
      "|\"Cooking with Sei...|it was what i nee...|A2SK6R2DPGY3P2|it was what i nee...|[it, was, what, i...|[needed, learning...|[needed learning,...|\n",
      "|\"Cooking with Sei...|\"As a vegetarian ...|A363UEL2VJNBX9|as a vegetarian i...|[as, a, vegetaria...|[vegetarian, alwa...|[vegetarian alway...|\n",
      "|\"Cooking with Sei...|Gluten has been a...|A3HRBWBN7DH9XV|gluten has been a...|[gluten, has, bee...|[gluten, wellkept...|[gluten wellkept,...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create word-level shingles (n-grams)\n",
    "def create_word_shingles(words, n=2):  \n",
    "    if len(words) < n:\n",
    "        return [\" \".join(words)]\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "# Define UDF\n",
    "shingle_udf = udf(lambda words: create_word_shingles(words, n=2), ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to generate shingles\n",
    "df_shingled = df_no_stop.withColumn(\"shingles\", shingle_udf(col(\"filtered_text\")))\n",
    "\n",
    "df_shingled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors (using hashing trick)\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=1 << 20)  # 2^20 features  \n",
    "featurized_data = hashing_tf.transform(df_shingled)   #each bigram gets hashed to an index between 0 and 1,048,575 and the value = freq of occurence (sparce vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|            features|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|[the, book, was, ...|[delivered, speed...|[delivered speedy...|(1048576,[5006,39...|\n",
      "|\"Cooking with Sei...|The Book of Seita...|A1I3R6RQ3XI76B|the book of seita...|[the, book, of, s...|[seitan, instruct...|[seitan instructi...|(1048576,[106369,...|\n",
      "|\"Cooking with Sei...|it was what i nee...|A2SK6R2DPGY3P2|it was what i nee...|[it, was, what, i...|[needed, learning...|[needed learning,...|(1048576,[128,687...|\n",
      "|\"Cooking with Sei...|\"As a vegetarian ...|A363UEL2VJNBX9|as a vegetarian i...|[as, a, vegetaria...|[vegetarian, alwa...|[vegetarian alway...|(1048576,[16324,3...|\n",
      "|\"Cooking with Sei...|Gluten has been a...|A3HRBWBN7DH9XV|gluten has been a...|[gluten, has, bee...|[gluten, wellkept...|[gluten wellkept,...|(1048576,[64571,9...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = featurized_data.withColumn(\"Review_ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHash LSH for Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinHash\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=6)\n",
    "model = mh.fit(df_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar reviews\n",
    "similar_reviews = model.approxSimilarityJoin(\n",
    "    df_with_id, df_with_id, 0.8, distCol=\"JaccardDistance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = similar_reviews.filter(col(\"datasetA.Review_ID\") != col(\"datasetB.Review_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only one direction of each pair\n",
    "filtered_df = filtered_df.filter(col(\"datasetA.review_ID\") < col(\"datasetB.review_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|Title_A                                                                          |Title_B                                      |review/text_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |review/text_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |JaccardDistance    |\n",
      "+---------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|High Fit, Low Fat Vegetarian                                                     |High Fit, Low Fat Vegetarian                 |I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The Eggplant Parmesan recipe is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too, I bought 10 as gifts for family members. Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The eggplant parmesan recipe (Vegetarian cookbook) is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too (I bought 10 as gifts for family members)! Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.08510638297872342|\n",
      "|The Frugal Gourmet Cooks Three Ancient Cuisines: China * Greece * Rome           |The Frugal Gourmet on Our Immigrant Ancestors|This is an excellent cook book and this is the handy PB edition! It's full of great recipes and stories by a very talented cook and writer. This one focuses on 3 major influences in the culinary world. Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!                                                                                                                                                                                                                                                                                                                  |My title blurb is a funny quote I remembered, Jeff Smith spoke on his entertaining PBS show. Before 'The Food Network' we had the witty and talented 'Frugal Gourmet'. This book deals with some simplistic, yet very good classic old world dishes. Nothing fancy, just great traditional food!This is yet another excellent cook book by Jeff Smith! It's full of great recipes and stories by a very talented cook and writer. This one focuses on old world cooking. I have used many of these recipes and found them to be very good. Being a home grown cook myself and having had many of my grandmother's classic recipes handed down to me, I found this book to be very helpful in expanding my culinary taste buds.Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!|0.42361111111111116|\n",
      "|Cooking with Too Hot Tamales : Recipes & Tips From Tv Food's Spiciest Cooking Duo|Mesa Mexicana                                |Mary Sue Miliken and Susan Feniger are two of the top chefs in LA. (And maybe the top female duo.) For those of us who grew up equating Mexican food with Tex-Mex, Miliken and Feniger's inventive take on traditional Mexican cuisine is a revelation. Their &quot;Border Grill&quot; in Santa Monica (4th and Broadway) is a noisy, splashy, foodie haven with superb drinks, a decent wine/beer list, and amazing food. For several years they also produced a fun and informational show on the Food network before that cable network went all-Emeril all-the-time. &quot;Cooking with Two Hot Tamales&quot; captures a lot of recipes and tips from the show. Many of the recipes herein one occasionally sees on Border Grill's menu. The house gucamole recipe is almost worth the price of the book on its own!As a cookbook, Two Hot Tamales is interesting, has an attractive layout, and, by the minimal standards of the genre, is well-written. Unlike their Mesa Mexicana, which I recomend only for the hobbyist chef with access to a decent Mexican grocer and time on his/her hands, Two Hot Tamales can be used on an everyday basis. Few of the recipes involve intensive prep work -- after all, they had to be prepared within the confines of a 30 minute TV show. Equally important for users outside the South-west, few of the recipes require specialized ingredients. Highly recommended.|Mary Sue Miliken and Susan Feniger are two of the top chefs in LA. Their &quot;Border Grill&quot; in Santa Monica (4th and Broadway) is a noisy, splashy, foodie haven with superb drinks, a decent wine/beer list, and amazing food. Mesa Mexicana offers recipes that one might easily see on Border Grill's menu. For those of us who grew up equating Mexican food with Tex-Mex, Miliken and Feniger's inventive take on traditional Mexican cuisine is a revelation.As a cookbook, Mesa Mexicana is interesting, has an attractive layout, and, by the minimal standards of the genre, is well-written. One would not want to use it on an everyday basis. Many of the recipes involve a fairly intensive amount of prep work and/or require specialized ingredients. For the hobbyist chef with access to a decent Mexican grocer and time on his/her hands, however, it is an inspiring and provocative work. Highly recommended on that qualified basis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |0.6363636363636364 |\n",
      "+---------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3 = (\n",
    "    filtered_df\n",
    "    .orderBy(col(\"JaccardDistance\").asc())\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "top3.select(\n",
    "    col(\"datasetA.Title\").alias(\"Title_A\"),\n",
    "    col(\"datasetB.Title\").alias(\"Title_B\"),\n",
    "    col(\"datasetA.review/text\").alias(\"review/text_A\"),\n",
    "    col(\"datasetB.review/text\").alias(\"review/text_B\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out pairs with Jaccard distance of 0\n",
    "filtered_df = filtered_df.filter(col(\"JaccardDistance\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|Title_A                                                                          |Title_B                                      |review/text_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |review/text_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |JaccardDistance    |\n",
      "+---------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|High Fit, Low Fat Vegetarian                                                     |High Fit, Low Fat Vegetarian                 |I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The Eggplant Parmesan recipe is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too, I bought 10 as gifts for family members. Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The eggplant parmesan recipe (Vegetarian cookbook) is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too (I bought 10 as gifts for family members)! Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.08510638297872342|\n",
      "|The Frugal Gourmet Cooks Three Ancient Cuisines: China * Greece * Rome           |The Frugal Gourmet on Our Immigrant Ancestors|This is an excellent cook book and this is the handy PB edition! It's full of great recipes and stories by a very talented cook and writer. This one focuses on 3 major influences in the culinary world. Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!                                                                                                                                                                                                                                                                                                                  |My title blurb is a funny quote I remembered, Jeff Smith spoke on his entertaining PBS show. Before 'The Food Network' we had the witty and talented 'Frugal Gourmet'. This book deals with some simplistic, yet very good classic old world dishes. Nothing fancy, just great traditional food!This is yet another excellent cook book by Jeff Smith! It's full of great recipes and stories by a very talented cook and writer. This one focuses on old world cooking. I have used many of these recipes and found them to be very good. Being a home grown cook myself and having had many of my grandmother's classic recipes handed down to me, I found this book to be very helpful in expanding my culinary taste buds.Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!|0.42361111111111116|\n",
      "|Cooking with Too Hot Tamales : Recipes & Tips From Tv Food's Spiciest Cooking Duo|Mesa Mexicana                                |Mary Sue Miliken and Susan Feniger are two of the top chefs in LA. (And maybe the top female duo.) For those of us who grew up equating Mexican food with Tex-Mex, Miliken and Feniger's inventive take on traditional Mexican cuisine is a revelation. Their &quot;Border Grill&quot; in Santa Monica (4th and Broadway) is a noisy, splashy, foodie haven with superb drinks, a decent wine/beer list, and amazing food. For several years they also produced a fun and informational show on the Food network before that cable network went all-Emeril all-the-time. &quot;Cooking with Two Hot Tamales&quot; captures a lot of recipes and tips from the show. Many of the recipes herein one occasionally sees on Border Grill's menu. The house gucamole recipe is almost worth the price of the book on its own!As a cookbook, Two Hot Tamales is interesting, has an attractive layout, and, by the minimal standards of the genre, is well-written. Unlike their Mesa Mexicana, which I recomend only for the hobbyist chef with access to a decent Mexican grocer and time on his/her hands, Two Hot Tamales can be used on an everyday basis. Few of the recipes involve intensive prep work -- after all, they had to be prepared within the confines of a 30 minute TV show. Equally important for users outside the South-west, few of the recipes require specialized ingredients. Highly recommended.|Mary Sue Miliken and Susan Feniger are two of the top chefs in LA. Their &quot;Border Grill&quot; in Santa Monica (4th and Broadway) is a noisy, splashy, foodie haven with superb drinks, a decent wine/beer list, and amazing food. Mesa Mexicana offers recipes that one might easily see on Border Grill's menu. For those of us who grew up equating Mexican food with Tex-Mex, Miliken and Feniger's inventive take on traditional Mexican cuisine is a revelation.As a cookbook, Mesa Mexicana is interesting, has an attractive layout, and, by the minimal standards of the genre, is well-written. One would not want to use it on an everyday basis. Many of the recipes involve a fairly intensive amount of prep work and/or require specialized ingredients. For the hobbyist chef with access to a decent Mexican grocer and time on his/her hands, however, it is an inspiring and provocative work. Highly recommended on that qualified basis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |0.6363636363636364 |\n",
      "+---------------------------------------------------------------------------------+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3 = (\n",
    "    filtered_df\n",
    "    .orderBy(col(\"JaccardDistance\").asc())\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "top3.select(\n",
    "    col(\"datasetA.Title\").alias(\"Title_A\"),\n",
    "    col(\"datasetB.Title\").alias(\"Title_B\"),\n",
    "    col(\"datasetA.review/text\").alias(\"review/text_A\"),\n",
    "    col(\"datasetB.review/text\").alias(\"review/text_B\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + Cosine LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "# exclude rows where 'filtered_text' has less than 6 words\n",
    "featurized_data = featurized_data.filter(size(col(\"filtered_text\")) >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IDF to get TF-IDF vectors\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "df_tfidf = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|            features|      tfidf_features|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|\"Cooking with Sei...|The book was deli...|A1FTPHKCHYAJTY|the book was deli...|[the, book, was, ...|[delivered, speed...|[delivered speedy...|(1048576,[5006,39...|(1048576,[5006,39...|\n",
      "|\"Cooking with Sei...|The Book of Seita...|A1I3R6RQ3XI76B|the book of seita...|[the, book, of, s...|[seitan, instruct...|[seitan instructi...|(1048576,[106369,...|(1048576,[106369,...|\n",
      "|\"Cooking with Sei...|it was what i nee...|A2SK6R2DPGY3P2|it was what i nee...|[it, was, what, i...|[needed, learning...|[needed learning,...|(1048576,[128,687...|(1048576,[128,687...|\n",
      "|\"Cooking with Sei...|\"As a vegetarian ...|A363UEL2VJNBX9|as a vegetarian i...|[as, a, vegetaria...|[vegetarian, alwa...|[vegetarian alway...|(1048576,[16324,3...|(1048576,[16324,3...|\n",
      "|\"Cooking with Sei...|Gluten has been a...|A3HRBWBN7DH9XV|gluten has been a...|[gluten, has, bee...|[gluten, wellkept...|[gluten wellkept,...|(1048576,[64571,9...|(1048576,[64571,9...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tfidf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = df_tfidf.withColumn(\"Review_ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH with Cosine-like projection (BucketedRandomProjectionLSH)\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.5, numHashTables=4)  #number of different ways you're looking for similarity.\n",
    "\n",
    "\n",
    "brp_model = brp.fit(df_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-join to find similar review pairs\n",
    "similar_pairs = brp_model.approxSimilarityJoin(df_with_id, df_with_id, threshold=5.0, distCol=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out duplicates and self-pairs\n",
    "similar_pairs_filtered = similar_pairs \\\n",
    "    .filter(col(\"datasetA.Review_ID\") < col(\"datasetB.Review_ID\")) \\\n",
    "    .filter(col(\"datasetA.Review_ID\") != col(\"datasetB.Review_ID\")) \\\n",
    "    .select(\"datasetA.Title\", \"datasetB.Title\", \"datasetA.User_id\", \"datasetB.User_id\", \"datasetA.review/text\", \"datasetB.review/text\", \"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o307.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 70) (100.65.113.22 executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:200)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:200)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msimilar_pairs_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m         },\n\u001b[0;32m    976\u001b[0m     )\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o307.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 70) (100.65.113.22 executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:200)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:292)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:200)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_filtered.orderBy(\"distance\").show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exlude pairs that are too similar (distance < 2.5)\n",
    "similar_pairs_filtered = similar_pairs_filtered.filter(col(\"distance\") > 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "|                                                                                         Title|                                                                                               Title|       User_id|       User_id|                                                                                         review/text|                                                                                         review/text|          distance|\n",
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "|                                                                  High Fit, Low Fat Vegetarian|                                                                        High Fit, Low Fat Vegetarian|A2JDVB1LKLN97N| AK9FPASRM79OX|I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both...|I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for b...|               2.0|\n",
      "|                                                       The All New All Purpose: Joy of Cooking|                                                All About Braising: The Art of Uncomplicated Cooking|A2PQTIF0WD7T3A| AO598PUMZ7CKU|                 My book arrived in a timely manner and as discribed. I would use this seller again.|Product arrived at a timely manner and the book is in good condition. Would purchase with the sam...|2.6457513110645907|\n",
      "|How to Grill: The Complete Illustrated Book of Barbecue Techniques, A Barbecue Bible! Cookbook|  This Can't Be Tofu!: 75 Recipes to Cook Something You Never Thought You Would--and Love Every Bite|A2ZPAJD0AJ0B66|A35NNPZP83H65F|                              this book is one of the best bq book on the market, i'm glad i got it.|            I'm so glad I got more recipes for tofu. It's so good for you. This is a wonderful book.|2.6457513110645907|\n",
      "|                      Jill Prescott's Ecole de Cuisine: Professional Cooking for the Home Chef|Saving Dinner the Low-Carb Way: Healthy Menus, Recipes, and the Shopping Lists That Will Keep the...|A1ZEHXQQLC9NB3|A191C036Y8W8CQ|This is a great cookbook. Everything I have made has been excellent. I have also given the book a...|           I have this and have also given it as gifts. For a busy family this is a great cool book.|2.6457513110645907|\n",
      "|                                                        Good Housekeeping Illustrated Cookbook|                                                       1000 Vegetarian Recipes From Around the World| A1ZGX2HV4L1JE| AYKWM682POY3J|                            this is best cookbook i have ever had. mine was falling apart. thank you|                     Whether you are vegetarian or not, this is the best cookbook I have ever owned.|2.6457513110645907|\n",
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_filtered.orderBy(\"distance\").show(5, truncate=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
